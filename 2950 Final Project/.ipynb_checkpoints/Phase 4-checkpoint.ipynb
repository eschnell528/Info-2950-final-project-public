{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf7698c",
   "metadata": {},
   "source": [
    "# Text Mining: A Data Driven Understanding of Marriage\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Before the internet became available to the public in 1993, most information was stored in books. The internet provided people with faster access to larger amounts of information and allowed for dissemination of digitized information on scales never seen before. As electronic books became popular due to their almost infinite ability to be shared, free digital libraries began to gain popularity. Project Gutenberg, the world’s first digital library, aims to preserve and spread some of history’s greatest novels. Project Guttenberg contains over 60,000 free ebooks that were deemed important enough to be curated and translated by volunteers. Accessing the information within Project Gutenberg could provide us with important lessons, and a better understanding of how society’s viewpoints changed over time.\n",
    "\n",
    "Despite the increased amount of available information, the ability to learn new information is limited by the rate at which that information can be processed. It is infeasible to read all 60,000+ of the great historical works stored in Project Gutenberg, illustrating the need for a new method that can be used to extract data from a large number of historic texts.\n",
    "\n",
    "I am interested in developing a text mining process that can be used to extract information about historical trends from Project Gutenberg. My text mining algorithm matches words against the world’s largest word emotion lexicon which classifies words as belonging to the categories Anger, Anticipation, Disgust, Fear, Joy, Negative, Positive, Sadness, Surprise, and Trust. For each book, I summed the scores associated with each category to generate 10 feature emotional scores that were used in my analysis. By picking a topic such as marriage or war, we can see how the perception of said topic changes over time in literature.\n",
    "\n",
    "To test my text mining methodology, I attempted to quantify how marriage changed over time. Marriage is an interesting topic to study because divorce rates have dramatically increased within the United States over the last 50 years and it is not clear if marriage is becoming less fulfilling, or if changing gender roles and expectations provide more ability to get a divorce. Additionally, there were around 200 books on marriage written in English and stored as UTF-8 text files on Project Gutenberg. This was beneficial as there was a reasonable number of books to analyze, but not so many that I could not handle that data set in time.\n",
    "\n",
    "In an attempt to quantify the changing nature of marriage and to determine the effectiveness of the text mining process I developed, I pose the following research questions: \n",
    "\n",
    "1) How do emotional traits associated with marriage change over time? \n",
    "\n",
    "2) Do males and females have different overall perceptions of marriage? \n",
    "\n",
    "3) Can the connotations associated with the title and historical factors such as war, fertility rates, and economic depression be used to predict changes in the perception of marriage?\n",
    "\n",
    "4) Are the traits associated with marriage different from the traits associated with other topics?\n",
    "\n",
    "Within this analysis a negative relationship between the year a book was published and the proportion of words with connotations of sadness, disgust, joy, or fear is found to be statistically significant. \n",
    "\n",
    "There was also a significant difference in the usage of words that were associated with surprise and joy in books that were written my male and female authors suggesting ____ may have been more ____ by marriage than ____. \n",
    "\n",
    "Models using regression and classification were tested using the tone of the title, year, and the aforementioned historical indicators to predict the occurrence of words with a set connotation and whether or not a book would use words with a certain connotation more than 40% of other books.\n",
    "\n",
    "The books on marriage had their features stripped and were clustered against books about war, an almost opposite topic. The two groups emerging from the clustering reveal that books on marriage and war do not cluster independently from each other, illustrating that the word occurrence in books in marriage and war may not really be that different from each other. This suggests that more advanced text mining may be needed before the described method becomes a valuable and consistent approach for analyzing historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c3b8f",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "### Data Source\n",
    "I obtained all of my data (178 books marriage and 20 books on war) from project Gutenberg: \n",
    "\n",
    "To see the books I chose from, go to advanced search and select: \n",
    "- Subject: marriage (or war)\n",
    "- Language: English\n",
    "- Filetype: Plain Text\n",
    "\n",
    "After clicking on a book, you get the option to open the text as a variety of filetypes. I chose UTF-8 text files.\n",
    "\n",
    "I excluded all duplicates and only considered the first book in a series, or the whole series as one text file, if possible, to keep one series from having too much of an effect on my data set. This reduced the number of viable books from 265 books to 178 books.\n",
    "\n",
    "https://www.gutenberg.org/ebooks/results/?author=&title=&subject=marriage&lang=en&category=&locc=&filetype=txt&submit_search=Search&pageno=1\n",
    "\n",
    "I looked up information on wikipedia to generate my predictors on war. \n",
    "\n",
    "I got the relevant war dates from: \n",
    "1) https://en.wikipedia.org/wiki/Napoleonic_Wars_casualties#cite_note-White_2014-15\n",
    "\n",
    "2) https://en.wikipedia.org/wiki/French_Revolutionary_Wars\n",
    "\n",
    "3) https://en.wikipedia.org/wiki/United_States_military_casualties_of_war\n",
    "\n",
    "4) https://en.wikipedia.org/wiki/United_Kingdom_casualties_of_war\n",
    "\n",
    "The war dates can be freely and ethically used as they are objective facts which cannot be copyrighted.\n",
    "\n",
    "The people making the wikipedia articles also do it so that the information can be shared.\n",
    "\n",
    "I used information from the following source to generate my predictors for fertility rates: https://www.gapminder.org/data/documentation/\n",
    "\n",
    "As a matter of time, I cannot match every writer with the birth rate of their nation within that time span. I calcuate the relevant birth rate by averaging the US, and English birth rate scores from 1-5 years as English and American writers make up the bulk of the authors. The link above provides data that is open source, but usage for detailed statistics is not recommended as the data is not robust enough for a complete study. As I only use the data to generate predictors, and only use data from countries that are well documented, I do not performe detailed statistical analysis using the data. The data from this source is robust and complete enough for my purposes, much more so that the US census bureau, and it is legally allowed for any academic purposes.\n",
    "\n",
    "\n",
    "### Lexicon Source\n",
    "The NRC Emotion Lexicon is self described as a \"list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).\" This list can be used for free for educational and research purposes.\n",
    "\n",
    "Here is the link to the website: https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
    "\n",
    "\n",
    "I chose this list because it is extremly comprehensive and contains scores for over 14,000 words. It is also a reputable source that has been used in several academic papers.\n",
    "\n",
    "I downloaded the lexicon as a CSV and modified it to obtain a reformatted lexicon that reduces my runtime. The process of reformatting the lexicon is described below.\n",
    "\n",
    "\n",
    "### How I Reformatted the Lexicon\n",
    "The CSV file contained over 140,000 rows containing:\n",
    "- the word of interest\n",
    "- the category\n",
    "- a 0 or 1\n",
    "\n",
    "The first word was the same in the first 10 rows so I reformatted the lexicon to store:\n",
    "- the word of interest\n",
    "- 10 numbers (a 0 or 1) for each category\n",
    "\n",
    "I did not add a reformatted row if all 10 numbers were 0's\n",
    "\n",
    "### How I Cleaned the Text Data\n",
    "\n",
    "I generated each row in my dataframe by adding 1 to each category attribute when a word with that category attribute appeared in the text.\n",
    "- I removed undesirable special characters and exploded the text into words\n",
    "- I removed unneccesary whitespace and removed common words without significant meaning to increase runtime\n",
    "- I compare the remaining words in the array with the reformatted lexicon and added the scores in the 10 categories of the lexicon to the overall score if a word in the lexicon appears in the list\n",
    "- I realized that counting the number of occurrances per word, and then comparing with the lexicon would decrease the runtime, but I noticed this too late to make the optimization\n",
    "- A friend who took data mining said this process I developed would be similar to vectorization if I implement the optimization, that I did not add (mentioned in the bullet above)\n",
    "\n",
    "### Limitations\n",
    "This study is extremely limited because:\n",
    "- It is based on a collection of 178 books that I picked because they were free to access. This means that the books may not be representative of the time period they were published in because I used the avaliable data to generate more volume instead of just picking the most representative books (If I had money to spend on this project, I would pick 20 representative books for each decade and use these as my data set)\n",
    "- There are periods of time that lack many data points while a lot of the dates of authorship seem to be from between 1800 - 1925. There is much less data from 1500-1799, meaning that there are significant gaps in my data.\n",
    "- The selected books were pick to be restored by volunteers in the 2010-2020's. This means that books that appeal to our current values may be selected for (the volunteers pick the books they like)\n",
    "- It relies on the accuracy of a lexicon that only scores words as having 0's or 1's per each category\n",
    "- It does not consider sarcasm, context, or the tendency for the meaning of words to change over time\n",
    "\n",
    "These limiations mean that we should not take the results of this study too seriously as the data has significant gaps (that had to be addressed by dropping data) and I did not have sufficient access to books to create a representative data set from which I could perform this study. Instead, this study should be seen as a precursor study that can be used to identify potential trends that could be investigated more thoroughly by a more extensive, better funded study. Nevertheless, the algorithms for collecting the data set and the analysis I plan to use could be used on a better data set to result in significant discoveries.\n",
    "\n",
    "Taking the results of this study too seriously and basing your decisions on whether or not to get married can result in a reduced quality of life. It is better to make these decisions for yourself while asking family members and professionals for advice. This analysis is meant to be purely academic and with the quality of the initial data set, it should not be taken too seriously.\n",
    "\n",
    "#### Motivation\n",
    "I created the dataset to accomplish my goal and answer the research questions listed above. There were not datasets that attempted to quantify the emotions of books on marriage in the past, so I had to text mine books in order to create one. I do not have time time to learn about each individual book and its importance, so I used project Gutenberg because it is supposed to contain books that were important enough to preserve from a diverse set of time periods.\n",
    "\n",
    "I created the dataset for my own use. There was no funding involved. My only motivation was answering my research questions and having the opportunity to teach myself how to text mine & improve my coding skills.\n",
    "\n",
    "\n",
    "#### Composition\n",
    "The instances that comprise the data set are books on marriage from project Gutenberg. Each row of the enclosed dataframe consists of:\n",
    "- A link to a book from Project Gutenberg on marriage encoded as a UTF-8 text file\n",
    "- The title of the book\n",
    "- The author's name\n",
    "- An estimated year of publication (or the midpoint of the author's life)\n",
    "- The author's gender\n",
    "- Word counters for the categories: \n",
    "    - 1) anger\n",
    "    - 2) anticipation\n",
    "    - 3) disgust\n",
    "    - 4) fear\n",
    "    - 5) joy\n",
    "    - 6) negative\n",
    "    - 7) positive\n",
    "    - 8) sadness\n",
    "    - 9) surprise\n",
    "    - 10) trust\n",
    "- The sum of all the word counters\n",
    "- The score of positive words in the title\n",
    "- The score of negative words in the title\n",
    "- The average fertility index of the United States and Britain over the year of interest and the 4 prior years\n",
    "- A score indicating whether or not Britain or the United States was engaged in a major war\n",
    "- The average score of the recessions the United States was in during the target year and 3 previous years \n",
    "\n",
    "I obtained all of my word count data (on 178 out of 265 books using the desired search parameters) from project Gutenberg. I excluded all duplicates and only considered the first book in a series, or the whole series as one text file, if possible, to keep one series from having too much of an effect on my data set. This reduced the number of viable books from 265 books to 178 books, so my data is technically from a larger set.\n",
    "\n",
    "The contents of each instance are clearly described within this section\n",
    "\n",
    "Elements of the data can serve as labels for each instance. The useful ones include the title of the book, the author's gender, and an estimated year of publication.\n",
    "\n",
    "There is no information missing from individual instances\n",
    "\n",
    "The data within single instances is stored within a dataframe so that it is easy to know what goes together.\n",
    "\n",
    "I have not determined the best data splits for training, validation, and testing. I want to review this topic a bit more by completing the new problem set before I develop the splits.\n",
    "\n",
    "There are no redundant instances within the data set. Some of the scores may be partially redundant, but I plan to address this using Principal Component Analysis. The sources of noise / errors within this data can be attributed to my estimate for year of publication. If the year of publication was not on project Gutenberg, I tried to estimate it by taking the median year in the author's life.\n",
    "\n",
    "None of the data is confidential as Project Gutenberg is open source and this project is not for financial gain.\n",
    "\n",
    "The data should not cause anxiety, but some of the book titles are offensive as the books were written in times when women and minorities had fewer rights. These books must be inluded because removing books with insulting titles from the analysis will significantly affect the results of this analysis.\n",
    "\n",
    "The data for fertility, war, and recessions is open source, avaliable to the public, and ethically sourced. I explain the procedures used to get the predictors as described in the appendicies.\n",
    "\n",
    "#### Collection process\n",
    "The process used for collecting the data associated with each instance is described above. Please see the sections Data Source, Lexicon Source, How I Reformatted the Lexicon, and How I Cleaned the Text Data for more thorough responces to the following questions.\n",
    "\n",
    "The data associated with each instance was obtained by concatenating information I typed out (Professor Mimno told me to do this) and the results of my algoriths (the 10 feature scores plus the total score). This information was all extracted out the txt page from Project Gutenberg.\n",
    "\n",
    "I made an algorithm to score the books as outlined above. This algorithm was not formally validated, but I tested it by running it on a text source and comparing it with the outputs from a vectorization based script my friend in data science made. I cannot include this information, but the code seems to work. \n",
    "\n",
    "The procedure for picking which books to consider is described above. Generally, I only use one book from a series and I choose the first one, or a compilation of volumes if it is included.\n",
    "\n",
    "I collected all of the data by finding it on the Project Gutenberg Pages or using my algorithm to extract it. I was not paid for doing this. \n",
    "\n",
    "The data was collected within a shorte timeframe and I confirmed that the books considered did not change over this time period. As of 10/10/22, the page I used to get txt book links was current.\n",
    "\n",
    "There were no ethical review processes conducted.\n",
    "\n",
    "None of the other questions are applicable.\n",
    "\n",
    "#### Preprocessing, Cleaning, and Labelling\n",
    "\n",
    "No instances of data were removed once they were generated. The data was labeled as described in 3.1, but this procedure was consistent throghout the entire process and planned in advance. \n",
    "\n",
    "I removed common words without meaning to reduce the runtime of my algorithm, but I made sure that none of these were in my lexicon to make sure that this had no effect upon my data.\n",
    "\n",
    "I also cleaned my lexicon, but this process did not result in the loss of any data that could effect my results as I restructured the data to run faster and removed words with all 0's as they cannot affect the results.\n",
    "\n",
    "None of the other questions are applicable.\n",
    "\n",
    "#### Uses\n",
    "\n",
    "This data set has not been used for other purposes. I created it very recently.\n",
    "\n",
    "The lexicon I adapt has been used as the basis of several papers. Here is a link to all the information on the origional lexicon: https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
    "\n",
    "This dataset is very specifically designed to answer the main questions I ask. I cannot think of other uses for it at the present time.\n",
    "\n",
    "The data set is viable for future uses as long as all parties using it recognize that it is current up to 12/09/22.\n",
    "\n",
    "Please see the section on limitations to know what the data set should not be used for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbbd54",
   "metadata": {},
   "source": [
    "# Preregistration Statement\n",
    "\n",
    "The tests described in the pre-registration statement help answer the research goals of 1) How do emotional traits associated with marriage change over time? and 2) Do males and females have different overall perceptions of marriage? The pre-registration statements, which are listed below, are incorporated into more rigorous testing statements that include all of the promised analyses.\n",
    "\n",
    "### I plan to perform a regression of 'Anticipation' scores versus time.\n",
    "\n",
    "Prediction 1: The 'Anticipation' score increases during the Women's Sufferage Movement from 1848 to 1917. \n",
    "\n",
    "Null hypothesis: beta_anticipation_time = 0\n",
    "\n",
    "Alternative hypothesis: beta_anticipation_time > 0\n",
    "\n",
    "### I plan to compare the 'Positive' scores of books written by male and female authors using a t-test.\n",
    "\n",
    "Prediction 2: Books written by male authors have higher 'Positive' scores than books written by female authors \n",
    "\n",
    "Null hypothesis: beta_positive_male = beta_positive_female\n",
    "\n",
    "Alternative hypothesis: beta_positive_male - beta_positive_female > 0 \n",
    "\n",
    "### The pre-registration statement was defined early in this project. Minor adjustments are stated below.\n",
    "\n",
    "The Women's Sufferage Movement started in 1848 (at the Seneca Falls Women's Rights Convention), and ending in 1917, according to most major sources. However, I believe that the underlying developments of unnamed activists who paved the way for the Seneca Falls Women's Rights Convention, over the previous 50 years were the real starters of the Women's Sufferage movement. Responses to the increased independence of women such as propoganda pushing the Cult of Domesticity, starting in 1820, illustrate that the Women's Sufferage Movement was already considered a threat by the patriarchy. Additionally, 1917 is an improper end date for the movement which transitioned into fighting against the legality of gender based discrimination until 1972. For this reason, I extend the relevant time frame to 1820 - 1972.\n",
    "\n",
    "In practice the significance of the Anticipation score was not effected by the usage of dates between 1848-1917 versus dates between 1820 - 1972, but using the more historically accurate dates for comparison is better. \n",
    "\n",
    "### As a more complete version of the pre-registration statements, I plan to perform the following analyses on all 10 attributes that were measured by data mining\n",
    "\n",
    "I plan to determine whether or not the the score changes over time.\n",
    "\n",
    "Prediction: The attribute score changes during the time frame from 1820 - 1972. \n",
    "\n",
    "Null hypothesis: beta_attribute_time = 0\n",
    "\n",
    "Alternative hypothesis: |beta_attribute_time| > 0\n",
    "\n",
    "I plan to determine whether or not books written by male and female authors use words associated with the attributes more or less often.\n",
    "\n",
    "Prediction 2: Books written by male authors have higher 'Positive' scores than books written by female authors \n",
    "\n",
    "Null hypothesis: beta_attribute_male = beta_attribute_female\n",
    "\n",
    "Alternative hypothesis: |beta_positive_male-beta_positive_female| > 0 \n",
    "\n",
    "**Both of the aformentioned statements are extensions of the pre-registration statments that do everything that the pre-registration statements promise, and more.**\n",
    "\n",
    "# Additional Tests\n",
    "\n",
    "The additional tests listed here are designed to answer the research questions 3) Can the connotations associated with the title and historical factors such as war, fertility rates, and economic depression be used to predict changes in the perception of marriage? and 4) Are the traits associated with marriage different from the traits associated with other topics?\n",
    "\n",
    "To answer question 3, I seek to use predictors (including the positive and negative word occurances in the title, the year of publication, and historic metrics including fertility rates, occurance of war, and prevalence of economic depression) to predict the number of times words associated with the 10 emotional attributes are used. I attempt to use a regression and compare the MAE of the testing set (generated using the model) with the MAE from a good naive guess (the MAE using the median of the training set).\n",
    "\n",
    "To answer question 4, I plan to use clustering to generate two groups, using data from books on marriage and books about war. If the two groups do not separate out using K-Means clustering (the clusters are approximatly equal in size to the number of books about war and marriage), the occurance of words across books on different topics may not be different, so seeing if two clusters of approximatly the same size show up is relevant for confirming that the current text mining approach is viable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14aa5e",
   "metadata": {},
   "source": [
    "# Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986becd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a63d55",
   "metadata": {},
   "source": [
    "I import the final data set which includes the 10 attribute scores and predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30654ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Number of Hits</th>\n",
       "      <th>Pos_Title</th>\n",
       "      <th>Neg_Title</th>\n",
       "      <th>Fertility</th>\n",
       "      <th>War</th>\n",
       "      <th>Recession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1841</td>\n",
       "      <td>m</td>\n",
       "      <td>2761</td>\n",
       "      <td>2782</td>\n",
       "      <td>2046</td>\n",
       "      <td>3674</td>\n",
       "      <td>1889</td>\n",
       "      <td>6044</td>\n",
       "      <td>7673</td>\n",
       "      <td>2424</td>\n",
       "      <td>1082</td>\n",
       "      <td>3429</td>\n",
       "      <td>16173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>m</td>\n",
       "      <td>1913</td>\n",
       "      <td>1683</td>\n",
       "      <td>1541</td>\n",
       "      <td>2187</td>\n",
       "      <td>1165</td>\n",
       "      <td>4011</td>\n",
       "      <td>4493</td>\n",
       "      <td>1931</td>\n",
       "      <td>717</td>\n",
       "      <td>2305</td>\n",
       "      <td>10517</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Gender  Anger  Anticipation  Disgust  Fear   Joy  Negative  Positive  \\\n",
       "0  1841      m   2761          2782     2046  3674  1889      6044      7673   \n",
       "1  1870      m   1913          1683     1541  2187  1165      4011      4493   \n",
       "\n",
       "   Sadness  Surprise  Trust  Number of Hits  Pos_Title  Neg_Title  Fertility  \\\n",
       "0     2424      1082   3429           16173          0          0      5.527   \n",
       "1     1931       717   2305           10517          0          1      4.988   \n",
       "\n",
       "   War  Recession  \n",
       "0  0.0       1.00  \n",
       "1  0.0       0.75  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Final Data.txt', sep='^', header = None)\n",
    "df.columns = ['Link', 'Title', 'Author', 'Year', 'Gender', 'Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Negative', 'Positive', 'Sadness', 'Surprise', 'Trust', 'Number of Hits','Pos_Title','Neg_Title','Fertility', 'War', 'Recession']\n",
    "data_df = df.drop(['Link','Title','Author'], axis=1)\n",
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85327756",
   "metadata": {},
   "source": [
    "I convert m or f in gender to numeric inputs and create a copy of my dataframe for safekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56286df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Number of Hits</th>\n",
       "      <th>Pos_Title</th>\n",
       "      <th>Neg_Title</th>\n",
       "      <th>Fertility</th>\n",
       "      <th>War</th>\n",
       "      <th>Recession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2761</td>\n",
       "      <td>2782</td>\n",
       "      <td>2046</td>\n",
       "      <td>3674</td>\n",
       "      <td>1889</td>\n",
       "      <td>6044</td>\n",
       "      <td>7673</td>\n",
       "      <td>2424</td>\n",
       "      <td>1082</td>\n",
       "      <td>3429</td>\n",
       "      <td>16173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1913</td>\n",
       "      <td>1683</td>\n",
       "      <td>1541</td>\n",
       "      <td>2187</td>\n",
       "      <td>1165</td>\n",
       "      <td>4011</td>\n",
       "      <td>4493</td>\n",
       "      <td>1931</td>\n",
       "      <td>717</td>\n",
       "      <td>2305</td>\n",
       "      <td>10517</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Gender  Anger  Anticipation  Disgust  Fear   Joy  Negative  Positive  \\\n",
       "0  1841     1.0   2761          2782     2046  3674  1889      6044      7673   \n",
       "1  1870     1.0   1913          1683     1541  2187  1165      4011      4493   \n",
       "\n",
       "   Sadness  Surprise  Trust  Number of Hits  Pos_Title  Neg_Title  Fertility  \\\n",
       "0     2424      1082   3429           16173          0          0      5.527   \n",
       "1     1931       717   2305           10517          0          1      4.988   \n",
       "\n",
       "   War  Recession  \n",
       "0  0.0       1.00  \n",
       "1  0.0       0.75  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert gender to numeric inputs \n",
    "data_df['Gender']=data_df['Gender'].map({'m': 1, 'f': 0})\n",
    "data_df.loc[34,'Gender'] = 1\n",
    "#This line is needed because there is an error with mapping in my data\n",
    "\n",
    "copy_df = data_df \n",
    "\n",
    "data_df.head(2)\n",
    "#I normalize the scores in data_df again so they range from 0 to 1, \n",
    "# but store a copy in copy_df to use in the random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c2f7c",
   "metadata": {},
   "source": [
    "Scores are normalized using the number of hits to account for books being different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67954124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Pos_Title</th>\n",
       "      <th>Neg_Title</th>\n",
       "      <th>Fertility</th>\n",
       "      <th>War</th>\n",
       "      <th>Recession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170717</td>\n",
       "      <td>0.172015</td>\n",
       "      <td>0.126507</td>\n",
       "      <td>0.227169</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.373709</td>\n",
       "      <td>0.474433</td>\n",
       "      <td>0.149879</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.212020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.181896</td>\n",
       "      <td>0.160027</td>\n",
       "      <td>0.146525</td>\n",
       "      <td>0.207949</td>\n",
       "      <td>0.110773</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>0.427213</td>\n",
       "      <td>0.183607</td>\n",
       "      <td>0.068175</td>\n",
       "      <td>0.219169</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Gender     Anger  Anticipation   Disgust      Fear       Joy  \\\n",
       "0  1841     1.0  0.170717      0.172015  0.126507  0.227169  0.116800   \n",
       "1  1870     1.0  0.181896      0.160027  0.146525  0.207949  0.110773   \n",
       "\n",
       "   Negative  Positive   Sadness  Surprise     Trust  Pos_Title  Neg_Title  \\\n",
       "0  0.373709  0.474433  0.149879  0.066902  0.212020          0          0   \n",
       "1  0.381383  0.427213  0.183607  0.068175  0.219169          0          1   \n",
       "\n",
       "   Fertility  War  Recession  \n",
       "0      5.527  0.0       1.00  \n",
       "1      4.988  0.0       0.75  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for b in data_df.columns:\n",
    "    if (b!='Year' and b!='Gender' and b!='Pos_Title' and b!='Neg_Title' and b!='Fertility' and b!='War' and b!='Recession'):\n",
    "        data_df[b] = data_df[b]/data_df['Number of Hits']\n",
    "\n",
    "data_df = data_df.drop(['Number of Hits'], axis=1)\n",
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f3a40",
   "metadata": {},
   "source": [
    "Z-normalization is used on the measured values to constrain them between 0 and 1 inclusive. This makes the numbers more pleasant to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeeb923b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Pos_Title</th>\n",
       "      <th>Neg_Title</th>\n",
       "      <th>Fertility</th>\n",
       "      <th>War</th>\n",
       "      <th>Recession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645769</td>\n",
       "      <td>0.509382</td>\n",
       "      <td>0.190096</td>\n",
       "      <td>0.872547</td>\n",
       "      <td>0.216841</td>\n",
       "      <td>0.521525</td>\n",
       "      <td>0.519842</td>\n",
       "      <td>0.302319</td>\n",
       "      <td>0.335475</td>\n",
       "      <td>0.400660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.764576</td>\n",
       "      <td>0.428925</td>\n",
       "      <td>0.367037</td>\n",
       "      <td>0.704106</td>\n",
       "      <td>0.172478</td>\n",
       "      <td>0.568413</td>\n",
       "      <td>0.269380</td>\n",
       "      <td>0.547827</td>\n",
       "      <td>0.352313</td>\n",
       "      <td>0.448849</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Gender     Anger  Anticipation   Disgust      Fear       Joy  \\\n",
       "0  1841     1.0  0.645769      0.509382  0.190096  0.872547  0.216841   \n",
       "1  1870     1.0  0.764576      0.428925  0.367037  0.704106  0.172478   \n",
       "\n",
       "   Negative  Positive   Sadness  Surprise     Trust  Pos_Title  Neg_Title  \\\n",
       "0  0.521525  0.519842  0.302319  0.335475  0.400660          0          0   \n",
       "1  0.568413  0.269380  0.547827  0.352313  0.448849          0          1   \n",
       "\n",
       "   Fertility  War  Recession  \n",
       "0      5.527  0.0       1.00  \n",
       "1      4.988  0.0       0.75  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Anger = (data_df['Anger']-min(data_df['Anger']))/(max(data_df['Anger'])-min(data_df['Anger']))\n",
    "Anticipation = (data_df['Anticipation']-min(data_df['Anticipation']))/(max(data_df['Anticipation'])-min(data_df['Anticipation']))\n",
    "Disgust = (data_df['Disgust']-min(data_df['Disgust']))/(max(data_df['Disgust'])-min(data_df['Disgust']))\n",
    "Fear = (data_df['Fear']-min(data_df['Fear']))/(max(data_df['Fear'])-min(data_df['Fear']))\n",
    "Joy = (data_df['Joy']-min(data_df['Joy']))/(max(data_df['Joy'])-min(data_df['Joy']))\n",
    "Negative = (data_df['Negative']-min(data_df['Negative']))/(max(data_df['Negative'])-min(data_df['Negative']))\n",
    "Positive = (data_df['Positive']-min(data_df['Positive']))/(max(data_df['Positive'])-min(data_df['Positive']))\n",
    "Sadness = (data_df['Sadness']-min(data_df['Sadness']))/(max(data_df['Sadness'])-min(data_df['Sadness']))\n",
    "Surprise = (data_df['Surprise']-min(data_df['Surprise']))/(max(data_df['Surprise'])-min(data_df['Surprise']))\n",
    "Trust = (data_df['Trust']-min(data_df['Trust']))/(max(data_df['Trust'])-min(data_df['Trust']))\n",
    "\n",
    "data_df['Anger'] = Anger\n",
    "data_df['Anticipation'] = Anticipation\n",
    "data_df['Disgust'] = Disgust\n",
    "data_df['Fear'] = Fear\n",
    "data_df['Joy'] = Joy\n",
    "data_df['Negative'] = Negative\n",
    "data_df['Positive'] = Positive\n",
    "data_df['Sadness'] = Sadness\n",
    "data_df['Surprise'] = Surprise\n",
    "data_df['Trust'] = Trust\n",
    "\n",
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e88858",
   "metadata": {},
   "source": [
    "Separate dataframes containing books written by male and female authors are generated (this helpful for answering research question 2: Do males and females have different overall perceptions of marriage?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e61bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df = data_df[data_df['Gender']==1]\n",
    "female_df = data_df[data_df['Gender']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38295a9",
   "metadata": {},
   "source": [
    "The data needed for the first 3 research questions has sucessfully been prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39628a",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "**Evaluation of significance and interpretation is included in this section**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1dec4",
   "metadata": {},
   "source": [
    "A correlation matrix can be used to identify when there is a strong relationship between variables, and the supposed nature of that relationship (the relationship is not neccesary causal).\n",
    "\n",
    "The correlation matrix below clearly shows that there are moderatly strong negative relationships between the year of publication and several of the 10 attributes. This indicates that there may be a trend of these attributes decreasing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fad0063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Correlation Matrix of Year and Scores')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(data_df[['Year', 'Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Negative', 'Positive', 'Sadness', 'Surprise', 'Trust']].corr())\n",
    "plt.title('Correlation Matrix of Year and Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16d761",
   "metadata": {},
   "source": [
    "Here are the correlation scores with year for each of the 10 attributes.\n",
    "\n",
    "It appears as if there could be a significant negative relationship between Year and Sadness, Disgust, Joy, Fear, and Positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4ba196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year            1.000000\n",
       "Anger          -0.063579\n",
       "Anticipation   -0.095985\n",
       "Disgust        -0.187012\n",
       "Fear           -0.165636\n",
       "Joy            -0.166112\n",
       "Negative       -0.020813\n",
       "Positive       -0.109650\n",
       "Sadness        -0.221648\n",
       "Surprise        0.046936\n",
       "Trust          -0.046330\n",
       "Name: Year, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[['Year', 'Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Negative', 'Positive', 'Sadness', 'Surprise', 'Trust']].corr()['Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e343c",
   "metadata": {},
   "source": [
    "An OLS regression creates a large number of linear regression models and compares them against randomly generated models with slope of 0 to figure out whether or not the equations obtained by bootstrapping are statistically different from those made using a slope of 0.\n",
    "\n",
    "OLS tests were performed below using one input variable which was the year, and the output was the normalized score for the feature\n",
    "\n",
    "The P values from the OLS regression indicate that there is a statistically significant negative relationship between Year and Sadness, Year and Disgust, Year and Jot, and Year and Fear as 0.003 < 0.014 < 0.029 < 0.030 < 0.050.\n",
    "\n",
    "Anticipation vs year yields a p value of 0.210 > 0.05, so anticipation does not significantly change over time.\n",
    "\n",
    "The OLS regressions were performed for the other attributes, but the results for the t-test component (all were determined to have p values of higher than 0.05) indicate that the relationship between year and the discussed feature were not significant. I show the output for the attribute with the lowest excluded p-value and Anticipation, which was discussed in the pre-registration, but decline to show all of the OLS summaries as these take up a lot of space without adding much value.\n",
    "\n",
    "Side node: while interpreting the OLS summaries, it is important to consider both the P value for year and the Adjusted R-squared value of the model. The really low R-squared values indicate that year alone is a poor predictor of the number of occurances of a word. This likely means we are underfitting the model and more predictors are needed. We will explore this phenomenon more after investigating differences in the male and female perception of marriage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e259ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Sadness   R-squared:                       0.049\n",
      "Model:                            OLS   Adj. R-squared:                  0.044\n",
      "Method:                 Least Squares   F-statistic:                     8.783\n",
      "Date:                Sat, 10 Dec 2022   Prob (F-statistic):            0.00348\n",
      "Time:                        11:45:01   Log-Likelihood:                 97.527\n",
      "No. Observations:                 172   AIC:                            -191.1\n",
      "Df Residuals:                     170   BIC:                            -184.8\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.8405      0.801      3.545      0.001       1.259       4.422\n",
      "Year          -0.0013      0.000     -2.964      0.003      -0.002      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                        6.318   Durbin-Watson:                   1.682\n",
      "Prob(Omnibus):                  0.042   Jarque-Bera (JB):               10.107\n",
      "Skew:                          -0.063   Prob(JB):                      0.00639\n",
      "Kurtosis:                       4.181   Cond. No.                     1.45e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.45e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Disgust   R-squared:                       0.035\n",
      "Model:                            OLS   Adj. R-squared:                  0.029\n",
      "Method:                 Least Squares   F-statistic:                     6.161\n",
      "Date:                Sat, 10 Dec 2022   Prob (F-statistic):             0.0140\n",
      "Time:                        11:45:01   Log-Likelihood:                 109.76\n",
      "No. Observations:                 172   AIC:                            -215.5\n",
      "Df Residuals:                     170   BIC:                            -209.2\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.2799      0.746      3.055      0.003       0.807       3.753\n",
      "Year          -0.0010      0.000     -2.482      0.014      -0.002      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                       26.014   Durbin-Watson:                   1.894\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               53.620\n",
      "Skew:                           0.690   Prob(JB):                     2.27e-12\n",
      "Kurtosis:                       5.361   Cond. No.                     1.45e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.45e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    Joy   R-squared:                       0.028\n",
      "Model:                            OLS   Adj. R-squared:                  0.022\n",
      "Method:                 Least Squares   F-statistic:                     4.824\n",
      "Date:                Sat, 10 Dec 2022   Prob (F-statistic):             0.0294\n",
      "Time:                        11:45:01   Log-Likelihood:                 75.830\n",
      "No. Observations:                 172   AIC:                            -147.7\n",
      "Df Residuals:                     170   BIC:                            -141.4\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.3425      0.909      2.577      0.011       0.548       4.137\n",
      "Year          -0.0011      0.000     -2.196      0.029      -0.002      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                       19.910   Durbin-Watson:                   1.842\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               24.798\n",
      "Skew:                           0.747   Prob(JB):                     4.12e-06\n",
      "Kurtosis:                       4.108   Cond. No.                     1.45e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.45e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   Fear   R-squared:                       0.027\n",
      "Model:                            OLS   Adj. R-squared:                  0.022\n",
      "Method:                 Least Squares   F-statistic:                     4.796\n",
      "Date:                Sat, 10 Dec 2022   Prob (F-statistic):             0.0299\n",
      "Time:                        11:45:01   Log-Likelihood:                 56.832\n",
      "No. Observations:                 172   AIC:                            -109.7\n",
      "Df Residuals:                     170   BIC:                            -103.4\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.8590      1.015      2.817      0.005       0.855       4.863\n",
      "Year          -0.0012      0.001     -2.190      0.030      -0.002      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                        9.999   Durbin-Watson:                   2.062\n",
      "Prob(Omnibus):                  0.007   Jarque-Bera (JB):               10.340\n",
      "Skew:                          -0.505   Prob(JB):                      0.00569\n",
      "Kurtosis:                       3.652   Cond. No.                     1.45e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.45e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Positive   R-squared:                       0.012\n",
      "Model:                            OLS   Adj. R-squared:                  0.006\n",
      "Method:                 Least Squares   F-statistic:                     2.069\n",
      "Date:                Sat, 10 Dec 2022   Prob (F-statistic):              0.152\n",
      "Time:                        11:45:01   Log-Likelihood:                 112.26\n",
      "No. Observations:                 172   AIC:                            -220.5\n",
      "Df Residuals:                     170   BIC:                            -214.2\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.5440      0.735      2.099      0.037       0.092       2.996\n",
      "Year          -0.0006      0.000     -1.438      0.152      -0.001       0.000\n",
      "==============================================================================\n",
      "Omnibus:                       18.934   Durbin-Watson:                   1.856\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               51.289\n",
      "Skew:                           0.376   Prob(JB):                     7.29e-12\n",
      "Kurtosis:                       5.568   Cond. No.                     1.45e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.45e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           Anticipation   R-squared:                       0.009\n",
      "Model:                            OLS   Adj. R-squared:                  0.003\n",
      "Method:                 Least Squares   F-statistic:                     1.581\n",
      "Date:                Sat, 10 Dec 2022   Prob (F-statistic):              0.210\n",
      "Time:                        11:45:01   Log-Likelihood:                 100.38\n",
      "No. Observations:                 172   AIC:                            -196.8\n",
      "Df Residuals:                     170   BIC:                            -190.5\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.4554      0.788      1.847      0.067      -0.100       3.011\n",
      "Year          -0.0005      0.000     -1.257      0.210      -0.001       0.000\n",
      "==============================================================================\n",
      "Omnibus:                       18.645   Durbin-Watson:                   2.240\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               67.116\n",
      "Skew:                           0.202   Prob(JB):                     2.67e-15\n",
      "Kurtosis:                       6.034   Cond. No.                     1.45e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.45e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "mlr = smf.ols(formula=\"Sadness ~ Year\", data=data_df).fit() #p = 0.003\n",
    "print(mlr.summary())\n",
    "mlr = smf.ols(formula=\"Disgust ~ Year\", data=data_df).fit() #p = 0.014\n",
    "print(mlr.summary())\n",
    "mlr = smf.ols(formula=\"Joy ~ Year\", data=data_df).fit() #p = 0.029\n",
    "print(mlr.summary())\n",
    "mlr = smf.ols(formula=\"Fear ~ Year\", data=data_df).fit() #p = 0.030\n",
    "print(mlr.summary())\n",
    "mlr = smf.ols(formula=\"Positive ~ Year\", data=data_df).fit() #p = 0.152\n",
    "print(mlr.summary())\n",
    "mlr = smf.ols(formula=\"Anticipation ~ Year\", data=data_df).fit() #p = 0.210\n",
    "print(mlr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580735c",
   "metadata": {},
   "source": [
    "Now we attempt to figure out whether or not there is a significant difference between the male and female experience of marriage by determining whether or not there is a significant difference between the normalized number of words that are associated with a particular attribute (anger, anticipation, disgust, fear, joy, negative, positive, sadness, surprise, and trust) in books written by male and female authors.\n",
    "\n",
    "Performing t-Tests for each attribute, we see that there is a significant difference in Joy and Surprise between males and females in marriage as (p_joy = 0.003 < p_surprise = 0.024 < 0.050. For the positive attribute which was initially discussed in the preregistration, p = 0.316 indicating that there is not a significant difference in the happiness of marriage between genders. A significant difference depending on male vs female authorship was not observed for the other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b2fb23d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: p = 0.774\n",
      "Anger: p = 0.694\n",
      "Anticipation: p = 0.099\n",
      "Disgust: p = 0.538\n",
      "Fear: p = 0.482\n",
      "Joy: p = 0.003\n",
      "Negative: p = 0.882\n",
      "Positive: p = 0.316\n",
      "Sadness: p = 0.245\n",
      "Surprise: p = 0.024\n",
      "Trust: p = 0.174\n"
     ]
    }
   ],
   "source": [
    "var = ['Year', 'Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Negative', 'Positive', 'Sadness', 'Surprise', 'Trust']\n",
    "\n",
    "for i in var:\n",
    "    print(str(i)+': p =',round(stats.ttest_ind(male_df[i],female_df[i])[1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cf425",
   "metadata": {},
   "source": [
    "Lets go back to discussing the OLS-linear regressions, which were previously used to determine whether or not a significant relationship existed between an attribute and the year.\n",
    "\n",
    "The adjusted R^2 values from before were really low indicating that year alone is a poor predictor of the shown scores as the model does not fit the data well. Performing a linear regression using multiple inputs (additional predictors) might allow for a higher R^2 value, indicating that the new model is better.\n",
    "\n",
    "To use multiple regression effectively, we should check to see if the predictors we want to add have a high level of multicollinearity. This can be done using VIF scores. A VIF score over 5 or 10 indicates a problem, while lower scores indicate that the data inputs can be used simultaneously to train most models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba45775a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predictor      Score\n",
      "0       Year  15.623402\n",
      "1     Gender   2.139805\n",
      "2  Pos_Title   1.354381\n",
      "3  Neg_Title   1.159299\n",
      "4  Fertility   5.803117\n",
      "5        War   1.494486\n",
      "6  Recession   7.295704\n"
     ]
    }
   ],
   "source": [
    "VIF_df = data_df[['Year','Gender','Pos_Title','Neg_Title','Fertility','War','Recession']]\n",
    "output = pd.DataFrame()\n",
    "output['Predictor'] = VIF_df.columns\n",
    "length = len(VIF_df.loc[0])\n",
    "Score = np.zeros(length)\n",
    "\n",
    "for i in range(length):\n",
    "    Score[i] = variance_inflation_factor(VIF_df.values, i)\n",
    "\n",
    "output['Score'] = Score\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31357bfa",
   "metadata": {},
   "source": [
    "The VIF scores indicate that multicollinearity between Year, Fertility, and Recession indicating that one of the three must be dropped.\n",
    "\n",
    "Removing year as a predictor may allow us to generate a better model while keeping the other two predictors. Let's see what the new VIF scores are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f107f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predictor     Score\n",
      "0     Gender  1.917703\n",
      "1  Pos_Title  1.335647\n",
      "2  Neg_Title  1.159286\n",
      "3  Fertility  3.721952\n",
      "4        War  1.210203\n",
      "5  Recession  3.753446\n"
     ]
    }
   ],
   "source": [
    "# Without year\n",
    "VIF_df = data_df[['Gender','Pos_Title','Neg_Title','Fertility','War','Recession']]\n",
    "output = pd.DataFrame()\n",
    "output['Predictor'] = VIF_df.columns\n",
    "length = len(VIF_df.loc[0])\n",
    "Score = np.zeros(length)\n",
    "\n",
    "for i in range(length):\n",
    "    Score[i] = variance_inflation_factor(VIF_df.values, i)\n",
    "\n",
    "output['Score'] = Score\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2efd89",
   "metadata": {},
   "source": [
    "Ee see that all the VIF scores have decreased to below the threshold of 5. They means that multicollinearity of predictors should not pose a significant issue while using linear regression with multiple inputs.\n",
    "\n",
    "Let's try using the predictors Gender, Pos_Title, Neg_Title, Fertility, War, and Recession to train an OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc1be169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Positive   R-squared:                       0.048\n",
      "Model:                            OLS   Adj. R-squared:                  0.013\n",
      "Method:                 Least Squares   F-statistic:                     1.381\n",
      "Date:                Sat, 10 Dec 2022   Prob (F-statistic):              0.225\n",
      "Time:                        11:45:01   Log-Likelihood:                 115.44\n",
      "No. Observations:                 172   AIC:                            -216.9\n",
      "Df Residuals:                     165   BIC:                            -194.8\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.4637      0.039     11.912      0.000       0.387       0.541\n",
      "Gender        -0.0192      0.019     -0.991      0.323      -0.057       0.019\n",
      "Pos_Title      0.0222      0.015      1.524      0.129      -0.007       0.051\n",
      "Neg_Title     -0.0417      0.024     -1.708      0.089      -0.090       0.007\n",
      "Fertility      0.0086      0.006      1.553      0.122      -0.002       0.019\n",
      "War           -0.0066      0.014     -0.468      0.641      -0.034       0.021\n",
      "Recession     -0.0010      0.034     -0.029      0.977      -0.068       0.066\n",
      "==============================================================================\n",
      "Omnibus:                       18.197   Durbin-Watson:                   1.855\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               47.396\n",
      "Skew:                           0.369   Prob(JB):                     5.11e-11\n",
      "Kurtosis:                       5.463   Cond. No.                         22.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "mlr = smf.ols(formula=\"Positive ~ Gender + Pos_Title + Neg_Title + Fertility + War + Recession\", data=data_df).fit()\n",
    "print(mlr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d084d",
   "metadata": {},
   "source": [
    "We observe that the adjusted R^2 score is 0.013 which is higher than before. Nevertheless, the R^2 value is really low. This is likely because all of the predictors are catigorical variables, and it is hard to generate good models for linear regression using just cateorical data (without the use of continuous data). \n",
    "\n",
    "Below we remove recession and fertility instead use year to ensure that multicollinearity does not pose an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ae2e9b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Positive   R-squared:                       0.051\n",
      "Model:                            OLS   Adj. R-squared:                  0.022\n",
      "Method:                 Least Squares   F-statistic:                     1.767\n",
      "Date:                Sat, 10 Dec 2022   Prob (F-statistic):              0.122\n",
      "Time:                        11:45:01   Log-Likelihood:                 115.68\n",
      "No. Observations:                 172   AIC:                            -219.4\n",
      "Df Residuals:                     166   BIC:                            -200.5\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.7692      0.744      2.380      0.018       0.301       3.237\n",
      "Year          -0.0007      0.000     -1.710      0.089      -0.001       0.000\n",
      "Gender        -0.0213      0.019     -1.105      0.271      -0.059       0.017\n",
      "Pos_Title      0.0212      0.014      1.466      0.145      -0.007       0.050\n",
      "Neg_Title     -0.0465      0.025     -1.892      0.060      -0.095       0.002\n",
      "War           -0.0069      0.013     -0.522      0.602      -0.033       0.019\n",
      "==============================================================================\n",
      "Omnibus:                       16.202   Durbin-Watson:                   1.865\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               40.251\n",
      "Skew:                           0.322   Prob(JB):                     1.82e-09\n",
      "Kurtosis:                       5.281   Cond. No.                     1.47e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.47e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "mlr = smf.ols(formula=\"Positive ~ Year + Gender + Pos_Title + Neg_Title + War\", data=data_df).fit()\n",
    "print(mlr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedaabcf",
   "metadata": {},
   "source": [
    "The observed adjusted R-squared value is 0.022 which is higher than the R^2 value which was calculated when year was omitted (0.013). Nevertheless, the two R-squared values are really low meaning that a better form of regression might be needed.\n",
    "\n",
    "**One option that could work well is a Random Forest Regressor model.**\n",
    "Random forest models build a stated number of decision trees in a pseudorandom manner and weight each of the decision trees based on their ability to produce good results, as determined using the training data. The decision trees are given weights based on how good they are and the sum of the weighted combinations is used as the final regression model (thus random forests are an ensemble method). Random forest regression models are good because they mean that you do not need to linearize the data yourself and can instead let the model do the work.\n",
    "\n",
    "As the random forest model is an ensemble method, there is no need to bootstrap as multiple decision trees are combined as a weighted average within the model.\n",
    "\n",
    "**An implementation of the random forest regressor is defined below.**\n",
    "\n",
    "inp_str is the name of the variable you are testing for (ex:\"Anticipation\")\n",
    "\n",
    "num_trees is the number of decision trees that are ensembled to create the regressor\n",
    "\n",
    "test_prop is the proportion of the data set that is used for testing\n",
    "\n",
    "val_prop is linearly correlated with the proportion of the data set that is used for validation\n",
    "\n",
    "df is the entered dataframe\n",
    "\n",
    "**it is important to include a validation set to track the effect of adjusting hyperparameters without unintentionally fitting to the testing set. I did not look at test results and commented this section out while adjusting the hyperparameters for inputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7486af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_reg(inp_str,num_trees,test_prop,val_prop,df):\n",
    "    y = np.array(df[inp_str])\n",
    "    df = df.drop(['Anger','Anticipation','Disgust','Fear','Joy','Negative','Positive','Sadness','Surprise','Trust'],axis=1)\n",
    "    df = np.array(df)\n",
    "    \n",
    "    filler_x, test_x, filler_y, test_y = train_test_split(df, y, test_size = test_prop)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(filler_x, filler_y, test_size = val_prop)\n",
    "    rf = RandomForestRegressor(num_trees)\n",
    "    rf.fit(train_x, train_y)\n",
    "    predictions_tr = rf.predict(train_x)\n",
    "    predictions_vl = rf.predict(val_x)\n",
    "    predictions_ts = rf.predict(test_x)\n",
    "\n",
    "    mae_test = np.mean(abs(predictions_ts-test_y))\n",
    "    mae_median_test = np.mean(abs(np.median(train_y)-test_y))\n",
    "\n",
    "    mae_val = np.mean(abs(predictions_vl-val_y))\n",
    "    mae_median_val = np.mean(abs(np.median(train_y)-val_y))\n",
    "    \n",
    "    mae_train = np.mean(abs(predictions_tr-train_y))\n",
    "    mae_median_train = np.mean(abs(np.median(train_y)-train_y))\n",
    "    \n",
    "    print('For ' + inp_str + ':')\n",
    "    print('Training MAE: ' + str(mae_train))\n",
    "    print('Dividing by the best naive statistical estimate: ' + str(mae_train/mae_median_train))\n",
    "\n",
    "    print('Validation MAE: ' + str(mae_val))\n",
    "    print('Dividing by the best naive statistical estimate: ' + str(mae_val/mae_median_val))\n",
    "    \n",
    "    print('Testing MAE: ' + str(mae_test))\n",
    "    print('Dividing by the best naive statistical estimate: ' + str(mae_test/mae_median_test))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    return(mae_test, mae_median_test, mae_val, mae_median_val, mae_train, mae_median_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2178050",
   "metadata": {},
   "source": [
    "Below we generate scores using the random forest regressor (100 decision trees used). For each attribute, the random forest regressor calculates the MAE of the training, validation, and testing sets. It also calculates the MAE using the median of the training set in place of the prediction from the model. This serves as the best naive predictor as the median is the single value that yields the minimum MAE for a data set, and in theory the test, train, and validation sets should be similarly composed as they are randomly separated from a single data set into 3 different data sets. \n",
    "\n",
    "For each attribute, the model returns the MAE for training, testing, and validation and each MAE divided by its corresponding statistical estimate. If MAE/(the naive MAE prediction) is greater than 1, the model is performing worse than the naive prediction. If the ratio is less than 1, the model is performing better, as the model's MAE is lower.\n",
    "\n",
    "If the MAE is consistently lower for the training (lower is better for MAE) than test and validation sets, overfitting is likely occurring. This is highly plausible as my data sets were not very big as I needed to mine a book to get each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5190d39f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Anger:\n",
      "Training MAE: 0.058294749212861686\n",
      "Dividing by the best naive statistical estimate: 0.4474986335552836\n",
      "Validation MAE: 0.17542620215378243\n",
      "Dividing by the best naive statistical estimate: 1.031402663994416\n",
      "Testing MAE: 0.14088014945836216\n",
      "Dividing by the best naive statistical estimate: 0.9630608335780577\n",
      "\n",
      "For Anticipation:\n",
      "Training MAE: 0.03870149873594193\n",
      "Dividing by the best naive statistical estimate: 0.4415623974503653\n",
      "Validation MAE: 0.09025358842210462\n",
      "Dividing by the best naive statistical estimate: 1.1525933426043964\n",
      "Testing MAE: 0.1451305200020766\n",
      "Dividing by the best naive statistical estimate: 1.1496404447106119\n",
      "\n",
      "For Disgust:\n",
      "Training MAE: 0.0382200354748587\n",
      "Dividing by the best naive statistical estimate: 0.45990640757620627\n",
      "Validation MAE: 0.1454139929708478\n",
      "Dividing by the best naive statistical estimate: 1.1163978165978763\n",
      "Testing MAE: 0.11453365740892311\n",
      "Dividing by the best naive statistical estimate: 1.006657443270642\n",
      "\n",
      "For Fear:\n",
      "Training MAE: 0.06671776271694804\n",
      "Dividing by the best naive statistical estimate: 0.45927871381684554\n",
      "Validation MAE: 0.11029710248460721\n",
      "Dividing by the best naive statistical estimate: 0.9268622304007323\n",
      "Testing MAE: 0.14490262679184768\n",
      "Dividing by the best naive statistical estimate: 1.0943126213771064\n",
      "\n",
      "For Joy:\n",
      "Training MAE: 0.056687095983052556\n",
      "Dividing by the best naive statistical estimate: 0.44312570258180456\n",
      "Validation MAE: 0.10920107374782938\n",
      "Dividing by the best naive statistical estimate: 0.846566742553155\n",
      "Testing MAE: 0.12289187574841257\n",
      "Dividing by the best naive statistical estimate: 1.2478027627854085\n",
      "\n",
      "For Negative:\n",
      "Training MAE: 0.055349636305855224\n",
      "Dividing by the best naive statistical estimate: 0.5188307668168511\n",
      "Validation MAE: 0.13695339413923652\n",
      "Dividing by the best naive statistical estimate: 1.0666799916846572\n",
      "Testing MAE: 0.12945828652824407\n",
      "Dividing by the best naive statistical estimate: 1.213634063788461\n",
      "\n",
      "For Positive:\n",
      "Training MAE: 0.04324271962054869\n",
      "Dividing by the best naive statistical estimate: 0.44315949869588644\n",
      "Validation MAE: 0.10846319318159192\n",
      "Dividing by the best naive statistical estimate: 1.2347441666116346\n",
      "Testing MAE: 0.09924236086830222\n",
      "Dividing by the best naive statistical estimate: 1.1458940655043037\n",
      "\n",
      "For Sadness:\n",
      "Training MAE: 0.034083179795702946\n",
      "Dividing by the best naive statistical estimate: 0.3566554011631212\n",
      "Validation MAE: 0.08521910674998448\n",
      "Dividing by the best naive statistical estimate: 1.0870307188573256\n",
      "Testing MAE: 0.12781389667637338\n",
      "Dividing by the best naive statistical estimate: 0.8627321820142223\n",
      "\n",
      "For Surprise:\n",
      "Training MAE: 0.051387282681253386\n",
      "Dividing by the best naive statistical estimate: 0.43685189148575143\n",
      "Validation MAE: 0.10817885470677933\n",
      "Dividing by the best naive statistical estimate: 0.8999169634064235\n",
      "Testing MAE: 0.14944320184581963\n",
      "Dividing by the best naive statistical estimate: 1.03720797792933\n",
      "\n",
      "For Trust:\n",
      "Training MAE: 0.03629093962714805\n",
      "Dividing by the best naive statistical estimate: 0.26498387589004574\n",
      "Validation MAE: 0.09396545984672462\n",
      "Dividing by the best naive statistical estimate: 0.7248815581262846\n",
      "Testing MAE: 0.0672673326807666\n",
      "Dividing by the best naive statistical estimate: 0.5389261347628289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Anger_ACC = random_forest_reg('Anger',100,0.2,0.2,data_df)\n",
    "Anticipation_ACC = random_forest_reg('Anticipation',100,0.2,0.2,data_df)\n",
    "Disgust_ACC = random_forest_reg('Disgust',100,0.2,0.2,data_df)\n",
    "Fear_ACC = random_forest_reg('Fear',100,0.2,0.2,data_df)\n",
    "Joy_ACC = random_forest_reg('Joy',100,0.2,0.2,data_df)\n",
    "Negative_ACC = random_forest_reg('Negative',100,0.2,0.2,data_df)\n",
    "Positive_ACC = random_forest_reg('Positive',100,0.2,0.2,data_df)\n",
    "Sadness_ACC = random_forest_reg('Sadness',100,0.2,0.2,data_df)\n",
    "Surprise_ACC = random_forest_reg('Surprise',100,0.2,0.2,data_df)\n",
    "Trust_ACC = random_forest_reg('Trust',100,0.2,0.2,data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53c6d3",
   "metadata": {},
   "source": [
    "The results above are randomly generated. Across multiple runs the values tend to remain similar to each other. The MAE scores and ratio of the MAE divided by the naive estimate for MAE are almost always lower for the training set than the testing and validation sets indicating that overfitting is occurring. Adjusting the hyperparameters, did not significantly reduce this problem, and an ensamble method was already used to reduce the effects of multicollinearity. Aquiring better predictors that are not dependent on the year, such as the number of years the author was married for or divorced for before he or she published the book would be beneficial, but collecting this data would take a large amount of time. Collecting more data might be the best solution for future experiments. This would require using text mining to study a topic such as war that had 10 times as many usable files in Project Gutenberg's Database as marriage.\n",
    "\n",
    "The random forest regressor model seems to perform slightly worse that the naive estimate (using the median of the training set) for most attributes. Nevertheless, the models generated using the random regressor model for trust and sadness are often better than the naive estimate. \n",
    "\n",
    "Overall, the random forest regressor overfits the training set and does not do an amazing job predicting values. One possible reason for this is that many catigorical data values are used to predict a non-binary output. Attempting to use a random forest classifier probably won't fix overfitting, but might increase the effectiveness of the model.\n",
    "\n",
    "A random forest classifier is defined with input variables that are similar to the random forest regressor. The additional value adj is the amount that the threshold is decreased from the median value. As the values were initially normalized using Z-score normalization, the median value has a score of 0.5. Using adj = 0.1 decreases the threshold to 0.4. If normalized values have scores of higher than 0.4, a score of 1 is assigned and the book is classified as using words with the given attribute a reasonable amount. If not, the book is assigned a score of 0 and is classified as using words associated with that attribute sparingly in comparison with other books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffac6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_cla(inp_str,num_trees,test_prop,val_prop,df,adj):\n",
    "    op = np.round_(np.array(df[inp_str])+adj,decimals=0)\n",
    "    df['Outputs'] = op\n",
    "    y = np.array(df['Outputs'])\n",
    "    df = df.drop(['Anger','Anticipation','Disgust','Fear','Joy','Negative','Positive','Sadness','Surprise','Trust','Outputs'],axis=1)\n",
    "    df = np.array(df)\n",
    "\n",
    "    filler_x, test_x, filler_y, test_y = train_test_split(df, y, test_size = test_prop)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(filler_x, filler_y, test_size = val_prop)\n",
    "    rf = RandomForestClassifier(num_trees)\n",
    "    rf.fit(train_x, train_y)\n",
    "    predictions_test = rf.predict(test_x)\n",
    "    predictions_val = rf.predict(val_x)\n",
    "    predictions_train = rf.predict(train_x)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(test_y)):\n",
    "        if(test_y[i]==1):\n",
    "            if(test_y[i]==predictions_test[i]):\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if(test_y[i]==predictions_test[i]):\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    \n",
    "    test_precision = tp/(tp+fp)\n",
    "    test_recall = tp/(tp+fn)\n",
    "    \n",
    "    if(test_precision > 0 and test_recall > 0):\n",
    "        test_f1 = 2/((1/test_precision)+(1/test_recall))\n",
    "    else:\n",
    "        test_f1 = 0\n",
    "\n",
    "    tp2 = 0\n",
    "    fp2 = 0\n",
    "    tn2 = 0\n",
    "    fn2 = 0\n",
    "    \n",
    "    for i in range(len(train_y)):\n",
    "        if(train_y[i]==1):\n",
    "            if(train_y[i]==predictions_train[i]):\n",
    "                tp2 += 1\n",
    "            else:\n",
    "                fp2 += 1\n",
    "        else:\n",
    "            if(train_y[i]==predictions_train[i]):\n",
    "                tn2 += 1\n",
    "            else:\n",
    "                fn2 += 1\n",
    "\n",
    "    train_precision = tp2/(tp2+fp2)\n",
    "    train_recall = tp2/(tp2+fn2)\n",
    "    if(train_precision > 0 and train_recall > 0):\n",
    "        train_f1 = 2/((1/train_precision)+(1/train_recall))\n",
    "    else:\n",
    "        train_f1 = 0\n",
    "\n",
    "    tp3 = 0\n",
    "    fp3 = 0\n",
    "    tn3 = 0\n",
    "    fn3 = 0\n",
    "    \n",
    "    for i in range(len(val_y)):\n",
    "        if(val_y[i]==1):\n",
    "            if(val_y[i]==predictions_val[i]):\n",
    "                tp3 += 1\n",
    "            else:\n",
    "                fp3 += 1\n",
    "        else:\n",
    "            if(val_y[i]==predictions_val[i]):\n",
    "                tn3 += 1\n",
    "            else:\n",
    "                fn3 += 1\n",
    "\n",
    "    val_precision = tp3/(tp3+fp3)\n",
    "    val_recall = tp3/(tp3+fn3)\n",
    "    if(val_precision > 0 and val_recall > 0):\n",
    "        val_f1 = 2/((1/val_precision)+(1/val_recall))\n",
    "    else:\n",
    "        val_f1 = 0\n",
    "\n",
    "\n",
    "    print('For ' + inp_str + ': f1 score is ' + str(round(train_f1,3)) + ' for the training data, ' + str(round(val_f1,3)) + ' for the validation data, and ' + str(round(test_f1,3)) + ' for the test data.')\n",
    "    return(train_precision,train_recall,train_f1,test_precision,test_recall,test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ea95b",
   "metadata": {},
   "source": [
    "The F1 scores for each attribute are calculated for training, testing, and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da841dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Anger: f1 score is 0.984 for the training data, 0.943 for the validation data, and 0.955 for the test data.\n",
      "For Anticipation: f1 score is 0.96 for the training data, 0.783 for the validation data, and 0.842 for the test data.\n",
      "For Disgust: f1 score is 0.933 for the training data, 0.757 for the validation data, and 0.585 for the test data.\n",
      "For Fear: f1 score is 0.995 for the training data, 0.943 for the validation data, and 0.923 for the test data.\n",
      "For Joy: f1 score is 0.946 for the training data, 0.25 for the validation data, and 0.25 for the test data.\n",
      "For Negative: f1 score is 0.978 for the training data, 0.809 for the validation data, and 0.852 for the test data.\n",
      "For Positive: f1 score is 0.959 for the training data, 0.833 for the validation data, and 0.778 for the test data.\n",
      "For Sadness: f1 score is 0.969 for the training data, 0.7 for the validation data, and 0.8 for the test data.\n",
      "For Surprise: f1 score is 0.947 for the training data, 0.92 for the validation data, and 0.903 for the test data.\n",
      "For Trust: f1 score is 0.864 for the training data, 0.4 for the validation data, and 0.444 for the test data.\n"
     ]
    }
   ],
   "source": [
    "num_trees=100\n",
    "adj = 0.1\n",
    "Anger = random_forest_cla('Anger',num_trees,0.2,0.2,data_df,adj)\n",
    "Anticipation = random_forest_cla('Anticipation',num_trees,0.2,0.2,data_df,adj)\n",
    "Disgust = random_forest_cla('Disgust',num_trees,0.2,0.2,data_df,adj)\n",
    "Fear = random_forest_cla('Fear',num_trees,0.2,0.2,data_df,adj)\n",
    "Joy = random_forest_cla('Joy',num_trees,0.2,0.2,data_df,adj)\n",
    "Negative = random_forest_cla('Negative',num_trees,0.2,0.2,data_df,adj)\n",
    "Positive = random_forest_cla('Positive',num_trees,0.2,0.2,data_df,adj)\n",
    "Sadness = random_forest_cla('Sadness',num_trees,0.2,0.2,data_df,adj)\n",
    "Surprise = random_forest_cla('Surprise',num_trees,0.2,0.2,data_df,adj)\n",
    "Trust = random_forest_cla('Trust',num_trees,0.2,0.2,data_df,adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d083e",
   "metadata": {},
   "source": [
    "A higher f1 score is desirable.\n",
    "\n",
    "As expected, the f1 scores are almost always higher for the training sets than for the test and validation sets which indicates that overfitting still occurs. This is after adjustment of hyperparameters (adj, num_trees) to yield higher F1 scores. An F1 score of about 0.58 is typically the value associated with random chance, and almost all of the values calculated were above 0.58, meaning that the model was better than guessing randomly.\n",
    "\n",
    "To clarify, this does not suggest that the produced classifier is good, or better than the regressor made previously, as I did not find a good naive statistical estimate for comparison. Discovering such a metric would \n",
    "\n",
    "Below I import and prepare the cleaned data from war books, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36beab24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.194790</td>\n",
       "      <td>0.174478</td>\n",
       "      <td>0.117139</td>\n",
       "      <td>0.239702</td>\n",
       "      <td>0.116319</td>\n",
       "      <td>0.396707</td>\n",
       "      <td>0.426165</td>\n",
       "      <td>0.183183</td>\n",
       "      <td>0.089194</td>\n",
       "      <td>0.197944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.191196</td>\n",
       "      <td>0.171472</td>\n",
       "      <td>0.135683</td>\n",
       "      <td>0.215883</td>\n",
       "      <td>0.074996</td>\n",
       "      <td>0.386780</td>\n",
       "      <td>0.398336</td>\n",
       "      <td>0.157253</td>\n",
       "      <td>0.078264</td>\n",
       "      <td>0.195976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Anger  Anticipation   Disgust      Fear       Joy  Negative  Positive  \\\n",
       "0  0.194790      0.174478  0.117139  0.239702  0.116319  0.396707  0.426165   \n",
       "1  0.191196      0.171472  0.135683  0.215883  0.074996  0.386780  0.398336   \n",
       "\n",
       "    Sadness  Surprise     Trust  \n",
       "0  0.183183  0.089194  0.197944  \n",
       "1  0.157253  0.078264  0.195976  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd = pd.read_csv('2950 Data War', sep=' ', header = None)\n",
    "wd.columns = ['Anger','Anticipation','Disgust','Fear','Joy','Negative','Positive','Sadness','Surprise','Trust','Hits']\n",
    "\n",
    "for b in wd.columns:\n",
    "    wd[b] = wd[b]/wd['Hits']\n",
    "\n",
    "wd = wd.drop(['Hits'], axis=1)\n",
    "wd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f5e547d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.662399</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654543</td>\n",
       "      <td>0.713505</td>\n",
       "      <td>0.479624</td>\n",
       "      <td>0.357096</td>\n",
       "      <td>0.533407</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.184973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.621125</td>\n",
       "      <td>0.973087</td>\n",
       "      <td>0.159652</td>\n",
       "      <td>0.395622</td>\n",
       "      <td>0.149638</td>\n",
       "      <td>0.413280</td>\n",
       "      <td>0.153828</td>\n",
       "      <td>0.267917</td>\n",
       "      <td>0.80883</td>\n",
       "      <td>0.162958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Anger  Anticipation   Disgust      Fear       Joy  Negative  Positive  \\\n",
       "0  0.662399      1.000000  0.000000  0.654543  0.713505  0.479624  0.357096   \n",
       "1  0.621125      0.973087  0.159652  0.395622  0.149638  0.413280  0.153828   \n",
       "\n",
       "    Sadness  Surprise     Trust  \n",
       "0  0.533407   1.00000  0.184973  \n",
       "1  0.267917   0.80883  0.162958  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Anger_War = (wd['Anger']-min(wd['Anger']))/(max(wd['Anger'])-min(wd['Anger']))\n",
    "Anticipation_War = (wd['Anticipation']-min(wd['Anticipation']))/(max(wd['Anticipation'])-min(wd['Anticipation']))\n",
    "Disgust_War = (wd['Disgust']-min(wd['Disgust']))/(max(wd['Disgust'])-min(wd['Disgust']))\n",
    "Fear_War = (wd['Fear']-min(wd['Fear']))/(max(wd['Fear'])-min(wd['Fear']))\n",
    "Joy_War = (wd['Joy']-min(wd['Joy']))/(max(wd['Joy'])-min(wd['Joy']))\n",
    "Negative_War = (wd['Negative']-min(wd['Negative']))/(max(wd['Negative'])-min(wd['Negative']))\n",
    "Positive_War = (wd['Positive']-min(wd['Positive']))/(max(wd['Positive'])-min(wd['Positive']))\n",
    "Sadness_War = (wd['Sadness']-min(wd['Sadness']))/(max(wd['Sadness'])-min(wd['Sadness']))\n",
    "Surprise_War = (wd['Surprise']-min(wd['Surprise']))/(max(wd['Surprise'])-min(wd['Surprise']))\n",
    "Trust_War = (wd['Trust']-min(wd['Trust']))/(max(wd['Trust'])-min(wd['Trust']))\n",
    "\n",
    "wd['Anger'] = Anger_War\n",
    "wd['Anticipation'] = Anticipation_War\n",
    "wd['Disgust'] = Disgust_War\n",
    "wd['Fear'] = Fear_War\n",
    "wd['Joy'] = Joy_War\n",
    "wd['Negative'] = Negative_War\n",
    "wd['Positive'] = Positive_War\n",
    "wd['Sadness'] = Sadness_War\n",
    "wd['Surprise'] = Surprise_War\n",
    "wd['Trust'] = Trust_War\n",
    "\n",
    "#wd['Label'] = 'War'\n",
    "wd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a925c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.645769</td>\n",
       "      <td>0.509382</td>\n",
       "      <td>0.190096</td>\n",
       "      <td>0.872547</td>\n",
       "      <td>0.216841</td>\n",
       "      <td>0.521525</td>\n",
       "      <td>0.519842</td>\n",
       "      <td>0.302319</td>\n",
       "      <td>0.335475</td>\n",
       "      <td>0.400660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.764576</td>\n",
       "      <td>0.428925</td>\n",
       "      <td>0.367037</td>\n",
       "      <td>0.704106</td>\n",
       "      <td>0.172478</td>\n",
       "      <td>0.568413</td>\n",
       "      <td>0.269380</td>\n",
       "      <td>0.547827</td>\n",
       "      <td>0.352313</td>\n",
       "      <td>0.448849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Anger  Anticipation   Disgust      Fear       Joy  Negative  Positive  \\\n",
       "0  0.645769      0.509382  0.190096  0.872547  0.216841  0.521525  0.519842   \n",
       "1  0.764576      0.428925  0.367037  0.704106  0.172478  0.568413  0.269380   \n",
       "\n",
       "    Sadness  Surprise     Trust  \n",
       "0  0.302319  0.335475  0.400660  \n",
       "1  0.547827  0.352313  0.448849  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mar_comp = data_df.drop(['Year','Gender','Pos_Title','Neg_Title','Fertility','War','Recession','Outputs'],axis=1)\n",
    "#mar_comp['Label'] = 'Marriage'\n",
    "mar_comp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89749a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.662399</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654543</td>\n",
       "      <td>0.713505</td>\n",
       "      <td>0.479624</td>\n",
       "      <td>0.357096</td>\n",
       "      <td>0.533407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.184973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.621125</td>\n",
       "      <td>0.973087</td>\n",
       "      <td>0.159652</td>\n",
       "      <td>0.395622</td>\n",
       "      <td>0.149638</td>\n",
       "      <td>0.413280</td>\n",
       "      <td>0.153828</td>\n",
       "      <td>0.267917</td>\n",
       "      <td>0.808830</td>\n",
       "      <td>0.162958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.728896</td>\n",
       "      <td>0.928632</td>\n",
       "      <td>0.182410</td>\n",
       "      <td>0.614779</td>\n",
       "      <td>0.611741</td>\n",
       "      <td>0.374748</td>\n",
       "      <td>0.417043</td>\n",
       "      <td>0.280376</td>\n",
       "      <td>0.811766</td>\n",
       "      <td>0.172494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.520614</td>\n",
       "      <td>0.847436</td>\n",
       "      <td>0.066792</td>\n",
       "      <td>0.375220</td>\n",
       "      <td>0.436505</td>\n",
       "      <td>0.512036</td>\n",
       "      <td>0.304666</td>\n",
       "      <td>0.466202</td>\n",
       "      <td>0.927257</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.834433</td>\n",
       "      <td>0.116527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>167</td>\n",
       "      <td>0.340443</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.415664</td>\n",
       "      <td>0.349116</td>\n",
       "      <td>0.412806</td>\n",
       "      <td>0.314973</td>\n",
       "      <td>0.584137</td>\n",
       "      <td>0.221375</td>\n",
       "      <td>0.285285</td>\n",
       "      <td>0.605312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>168</td>\n",
       "      <td>0.588833</td>\n",
       "      <td>0.367640</td>\n",
       "      <td>0.672212</td>\n",
       "      <td>0.524219</td>\n",
       "      <td>0.373983</td>\n",
       "      <td>0.714983</td>\n",
       "      <td>0.310859</td>\n",
       "      <td>0.415888</td>\n",
       "      <td>0.583757</td>\n",
       "      <td>0.447212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>169</td>\n",
       "      <td>0.619979</td>\n",
       "      <td>0.447779</td>\n",
       "      <td>0.398445</td>\n",
       "      <td>0.591252</td>\n",
       "      <td>0.218778</td>\n",
       "      <td>0.522568</td>\n",
       "      <td>0.417831</td>\n",
       "      <td>0.415062</td>\n",
       "      <td>0.649687</td>\n",
       "      <td>0.189261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>170</td>\n",
       "      <td>0.706611</td>\n",
       "      <td>0.408350</td>\n",
       "      <td>0.375476</td>\n",
       "      <td>0.689065</td>\n",
       "      <td>0.263429</td>\n",
       "      <td>0.507607</td>\n",
       "      <td>0.564629</td>\n",
       "      <td>0.560722</td>\n",
       "      <td>0.428099</td>\n",
       "      <td>0.369857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>171</td>\n",
       "      <td>0.675837</td>\n",
       "      <td>0.456617</td>\n",
       "      <td>0.526850</td>\n",
       "      <td>0.676076</td>\n",
       "      <td>0.387906</td>\n",
       "      <td>0.585153</td>\n",
       "      <td>0.560361</td>\n",
       "      <td>0.528497</td>\n",
       "      <td>0.635481</td>\n",
       "      <td>0.340397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index     Anger  Anticipation   Disgust      Fear       Joy  Negative  \\\n",
       "0        0  0.662399      1.000000  0.000000  0.654543  0.713505  0.479624   \n",
       "1        1  0.621125      0.973087  0.159652  0.395622  0.149638  0.413280   \n",
       "2        2  0.728896      0.928632  0.182410  0.614779  0.611741  0.374748   \n",
       "3        3  0.520614      0.847436  0.066792  0.375220  0.436505  0.512036   \n",
       "4        4  0.032895      0.000000  0.834433  0.116527  0.000000  1.000000   \n",
       "..     ...       ...           ...       ...       ...       ...       ...   \n",
       "187    167  0.340443      0.401700  0.415664  0.349116  0.412806  0.314973   \n",
       "188    168  0.588833      0.367640  0.672212  0.524219  0.373983  0.714983   \n",
       "189    169  0.619979      0.447779  0.398445  0.591252  0.218778  0.522568   \n",
       "190    170  0.706611      0.408350  0.375476  0.689065  0.263429  0.507607   \n",
       "191    171  0.675837      0.456617  0.526850  0.676076  0.387906  0.585153   \n",
       "\n",
       "     Positive   Sadness  Surprise     Trust  \n",
       "0    0.357096  0.533407  1.000000  0.184973  \n",
       "1    0.153828  0.267917  0.808830  0.162958  \n",
       "2    0.417043  0.280376  0.811766  0.172494  \n",
       "3    0.304666  0.466202  0.927257  0.000000  \n",
       "4    0.000000  0.001155  0.000000  0.000041  \n",
       "..        ...       ...       ...       ...  \n",
       "187  0.584137  0.221375  0.285285  0.605312  \n",
       "188  0.310859  0.415888  0.583757  0.447212  \n",
       "189  0.417831  0.415062  0.649687  0.189261  \n",
       "190  0.564629  0.560722  0.428099  0.369857  \n",
       "191  0.560361  0.528497  0.635481  0.340397  \n",
       "\n",
       "[192 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_df = wd.append(mar_comp)\n",
    "comp_df = comp_df.reset_index()\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377ead2",
   "metadata": {},
   "source": [
    "There are 20 books on war. As there is no cluster with close to 20 zeros or 20 ones, the two types of books did not cluster separatly. This indcates the method of data mining old books is extremely limited in use.\n",
    "\n",
    "The number of zeros indicates the number of books in one cluster and the number of ones indicates the number of books in the other cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98804003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#I did not set a random seed for this one\n",
    "clustering = KMeans(2)\n",
    "output = clustering.fit(comp_df)\n",
    "clust = []\n",
    "\n",
    "for i in range(comp_df.shape[0]):\n",
    "    clust.append(output.labels_[i])\n",
    "\n",
    "print(clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05eb06",
   "metadata": {},
   "source": [
    "I set random seeds below to show the two possibilites for clustering which are actually the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e34ef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "clustering = KMeans(2)\n",
    "output = clustering.fit(comp_df)\n",
    "clust = []\n",
    "\n",
    "for i in range(comp_df.shape[0]):\n",
    "    clust.append(output.labels_[i])\n",
    "\n",
    "print(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5b7cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "random.seed(4)\n",
    "clustering = KMeans(2)\n",
    "output = clustering.fit(comp_df)\n",
    "clust = []\n",
    "\n",
    "for i in range(comp_df.shape[0]):\n",
    "    clust.append(output.labels_[i])\n",
    "\n",
    "print(clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53421d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The procedure of drawing conclusions off of data using the text mining algorithm I developed does not seem to be effective. \n",
    "\n",
    "Thus, the relationships we detected hold little merit.\n",
    "\n",
    "I am a little stuck on this part. Do you have any suggestions?\n",
    "\n",
    "# Limitations\n",
    "\n",
    "In adition to what is discussed in the conclusions, this study is extremely limited because:\n",
    "- It is based on a collection of 178 books that I picked because they were free to access. This means that the books may not be representative of the time period they were published in because I used the avaliable data to generate more volume instead of just picking the most representative books (If I had money to spend on this project, I would pick 20 representative books for each decade and use these as my data set)\n",
    "- There are periods of time that lack many data points while a lot of the dates of authorship seem to be from between 1800 - 1925. There is much less data from 1500-1799, meaning that there are significant gaps in my data.\n",
    "- The selected books were pick to be restored by volunteers in the 2010-2020's. This means that books that appeal to our current values may be selected for (the volunteers pick the books they like)\n",
    "- It relies on the accuracy of a lexicon that only scores words as having 0's or 1's per each category\n",
    "- It does not consider sarcasm, context, or the tendency for the meaning of words to change over time\n",
    "\n",
    "These limiations mean that we should not take the results of this study too seriously as the data has significant gaps (that had to be addressed by dropping data) and I did not have sufficient access to books to create a representative data set from which I could perform this study. Instead, this study should be seen as a precursor study that can be used to identify potential trends that could be investigated more thoroughly by a more extensive, better funded study. Nevertheless, the algorithms for collecting the data set and the analysis I plan to use could be used on a better data set to result in significant discoveries.\n",
    "\n",
    "As a matter of time, I could not match every writer with the fertility rate of their nation within that time span. I calcuate the relevant birth rate by averaging the US, and English birth rate scores from 1-5 years as English and American writers make up the bulk of the authors. I got the data from https://www.gapminder.org/data/documentation/ which is open source. Usage for detailed statistics is not recommended as the data is not robust enough for a complete study, but it is legally allowed and robust enough for my purposes. Same for war and recession.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e896488",
   "metadata": {},
   "source": [
    "# Source Code\n",
    "https://github.com/eschnell528/INFO-2950-Final-Project.git \n",
    "I am not sure if the link will work. I tried to get help in office hours. I was told to make it private. I would be happy to meet and walk you through my source code if this doesn't work. I would also be happy to email you a folder containing my code. My email is es796@cornell.edu\n",
    "\n",
    "# Acknowledgements\n",
    "\n",
    "I would like to thank Kefan Lu and Scarlett Wang for helping me understand statistics and ML models that I would not have figured out on my own\n",
    "\n",
    "# Questions for the Reviewer\n",
    "\n",
    "Do my choices of models make sense?\n",
    "\n",
    "Are there any more models or tests I should include?\n",
    "\n",
    "Would classification be more accurate than regression?\n",
    "\n",
    "Do you have any suggestions for my conclusion?\n",
    "\n",
    "# Appendix: Data Cleaning\n",
    "\n",
    "https://github.com/eschnell528/INFO-2950-Final-Project.git \n",
    "I am not sure if the link will work. I tried to get help in office hours. I was told to make it private. I would be happy to meet and walk you through my data cleaning if this doesn't work. I would also be happy to email you a folder containing my code. My email is es796@cornell.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09955d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
